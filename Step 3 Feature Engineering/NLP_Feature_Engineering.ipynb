{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Acquization -> Text Preprocess -> Feature Engineering"
      ],
      "metadata": {
        "id": "lf_f0THqvTvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Feature Engineering [Text Extraction/ Text Representation] from text?\n",
        "- Text extraction means finding and picking out useful parts from a big text.\n",
        "-Text representation is about turning text into numbers or patterns that a computer can work with.\n",
        "\n",
        "2. Why do we need it?\n",
        "- We need text extraction and text representation in NLP because computers canâ€™t understand text the way humans do. These techniques help us turn raw text into a format that computers can process and make sense of.\n",
        "\n",
        "3. Why is it difficult?\n",
        "- Images  to numbers is easy => bcz pixels is built from numbers\n",
        "- Audio to number is easy -> based on there frequency\n",
        "\n",
        "- text to number is diffcult\n",
        "\n",
        "4. Technique:\n",
        "- one hot encoding\n",
        "- bag of words\n",
        "- Ngrams\n",
        "- Tf-Idf\n",
        "- word2vec [Embeddings]\n",
        "- Custom feature's"
      ],
      "metadata": {
        "id": "-bw-HzKHtEGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. One Hot Encoding\n",
        "\n",
        "One-Hot Encoding (Simple Explanation)\n",
        "One-hot encoding is a way to convert words (or categories) into numbers that a computer can understand. Here's how it works:\n",
        "\n",
        "List all the unique words/categories you have.\n",
        "Give each word a unique position in a list.\n",
        "Represent each word as a list of 0s, except for a 1 in its unique position.\n",
        "Small Example:\n",
        "For the words: [\"apple\", \"banana\", \"cherry\"]\n",
        "\n",
        "\"apple\" becomes [1, 0, 0]\n",
        "\"banana\" becomes [0, 1, 0]\n",
        "\"cherry\" becomes [0, 0, 1]\n",
        "The position of the 1 shows which word it represents."
      ],
      "metadata": {
        "id": "lFs9EKaW3rUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flaws of One-Hot Encoding\n",
        "- Sparsity:\n",
        "The encoded lists can become very long if there are many unique words, but most of the list will be 0s. This wastes space and slows processing.\n",
        "\n",
        "- No Fixed Size:\n",
        "If you add new words to your data, the list size changes, so the encoding doesn't work well with new data unless updated.\n",
        "\n",
        "- OOV (Out of Vocabulary):\n",
        "If a word isn't in your original list, you can't represent it. For example, if \"orange\" wasn't in your list, the computer won't know what it is.\n",
        "\n",
        "- No Capturing of Semantic Meaning:\n",
        "One-hot encoding doesn't show relationships between words. For example, \"apple\" and \"banana\" are both fruits, but their encodings [1, 0, 0] and [0, 1, 0] don't reflect that.\n",
        "\n",
        "This is why next techniques used Bag of words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DNC2nghKdjIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQynJQ_pr69A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05d3532-8008-4a74-c01d-f45cd8579cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoded Data:\n",
            "   x0_apple  x0_banana  x0_cherry\n",
            "0       1.0        0.0        0.0\n",
            "1       0.0        1.0        0.0\n",
            "2       0.0        0.0        1.0\n",
            "3       1.0        0.0        0.0\n"
          ]
        }
      ],
      "source": [
        "# code\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Create a dataset of categories (words)\n",
        "data = ['apple', 'banana', 'cherry', 'apple']\n",
        "\n",
        "# Step 2: Reshape data to fit the encoder's input format\n",
        "data_reshaped = [[word] for word in data]\n",
        "\n",
        "# Step 3: Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)  # sparse=False to get a dense array\n",
        "\n",
        "# Step 4: Fit and transform the data\n",
        "one_hot = encoder.fit_transform(data_reshaped)\n",
        "\n",
        "# Step 5: Display the results\n",
        "encoded_df = pd.DataFrame(one_hot, columns=encoder.get_feature_names_out())\n",
        "print(\"One-Hot Encoded Data:\")\n",
        "print(encoded_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Bag of words"
      ],
      "metadata": {
        "id": "rvr8JYgKeWmZ"
      }
    }
  ]
}